# LLM Configuration Template
# Copy this to .env or set these as environment variables

# For LocalAI or TextGen WebUI (default):
LLM_ENDPOINT=http://localhost:8080/v1/completions
LLM_MODEL=llama-3

# For Ollama (uncomment and use this instead):
# LLM_ENDPOINT=http://localhost:11434/api/generate
# LLM_MODEL=llama3

LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=2048
LLM_STRICT_JSON=true

